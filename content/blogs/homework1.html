---
title: "Session 2: Homework 1"
author: "Group 12"
date: "2022-09-19"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    code_folding: show
---



<div id="rents-in-san-francisco-2000-2018" class="section level1">
<h1>Rents in San Francisco 2000-2018</h1>
<p>We are analysing a dataset of Craiglist listings for rental properties in the greater SF area.
The data dictionary is as follows</p>
<table>
<thead>
<tr class="header">
<th>variable</th>
<th>class</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>post_id</td>
<td>character</td>
<td>Unique ID</td>
</tr>
<tr class="even">
<td>date</td>
<td>double</td>
<td>date</td>
</tr>
<tr class="odd">
<td>year</td>
<td>double</td>
<td>year</td>
</tr>
<tr class="even">
<td>nhood</td>
<td>character</td>
<td>neighborhood</td>
</tr>
<tr class="odd">
<td>city</td>
<td>character</td>
<td>city</td>
</tr>
<tr class="even">
<td>county</td>
<td>character</td>
<td>county</td>
</tr>
<tr class="odd">
<td>price</td>
<td>double</td>
<td>price in USD</td>
</tr>
<tr class="even">
<td>beds</td>
<td>double</td>
<td>n of beds</td>
</tr>
<tr class="odd">
<td>baths</td>
<td>double</td>
<td>n of baths</td>
</tr>
<tr class="even">
<td>sqft</td>
<td>double</td>
<td>square feet of rental</td>
</tr>
<tr class="odd">
<td>room_in_apt</td>
<td>double</td>
<td>room in apartment</td>
</tr>
<tr class="even">
<td>address</td>
<td>character</td>
<td>address</td>
</tr>
<tr class="odd">
<td>lat</td>
<td>double</td>
<td>latitude</td>
</tr>
<tr class="even">
<td>lon</td>
<td>double</td>
<td>longitude</td>
</tr>
<tr class="odd">
<td>title</td>
<td>character</td>
<td>title of listing</td>
</tr>
<tr class="even">
<td>descr</td>
<td>character</td>
<td>description</td>
</tr>
<tr class="odd">
<td>details</td>
<td>character</td>
<td>additional details</td>
</tr>
</tbody>
</table>
<p>The dataset was used in a recent <a href="https://github.com/rfordatascience/tidytuesday">tidyTuesday</a> project, which is where we sourced the data.</p>
<pre class="r"><code># download directly off tidytuesdaygithub repo

rent &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-05/rent.csv&#39;)</code></pre>
<p>We have conducted an analysis on the variables. View the analysis below.</p>
<pre class="r"><code>skimr::skim(rent)</code></pre>
<table>
<caption>(#tab:skim_data)Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">rent</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">200796</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">17</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">9</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<colgroup>
<col width="18%" />
<col width="13%" />
<col width="18%" />
<col width="5%" />
<col width="8%" />
<col width="8%" />
<col width="12%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">post_id</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">9</td>
<td align="right">14</td>
<td align="right">0</td>
<td align="right">200796</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">nhood</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">4</td>
<td align="right">43</td>
<td align="right">0</td>
<td align="right">167</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">city</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">5</td>
<td align="right">19</td>
<td align="right">0</td>
<td align="right">104</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">county</td>
<td align="right">1394</td>
<td align="right">0.99</td>
<td align="right">4</td>
<td align="right">13</td>
<td align="right">0</td>
<td align="right">10</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">address</td>
<td align="right">196888</td>
<td align="right">0.02</td>
<td align="right">1</td>
<td align="right">38</td>
<td align="right">0</td>
<td align="right">2869</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">title</td>
<td align="right">2517</td>
<td align="right">0.99</td>
<td align="right">2</td>
<td align="right">298</td>
<td align="right">0</td>
<td align="right">184961</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">descr</td>
<td align="right">197542</td>
<td align="right">0.02</td>
<td align="right">13</td>
<td align="right">16975</td>
<td align="right">0</td>
<td align="right">3025</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">details</td>
<td align="right">192780</td>
<td align="right">0.04</td>
<td align="right">4</td>
<td align="right">595</td>
<td align="right">0</td>
<td align="right">7667</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="12%" />
<col width="8%" />
<col width="12%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">date</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.01e+07</td>
<td align="right">44694.07</td>
<td align="right">2.00e+07</td>
<td align="right">2.01e+07</td>
<td align="right">2.01e+07</td>
<td align="right">2.01e+07</td>
<td align="right">2.02e+07</td>
<td align="left">▁▇▁▆▃</td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.01e+03</td>
<td align="right">4.48</td>
<td align="right">2.00e+03</td>
<td align="right">2.00e+03</td>
<td align="right">2.01e+03</td>
<td align="right">2.01e+03</td>
<td align="right">2.02e+03</td>
<td align="left">▁▇▁▆▃</td>
</tr>
<tr class="odd">
<td align="left">price</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.14e+03</td>
<td align="right">1427.75</td>
<td align="right">2.20e+02</td>
<td align="right">1.30e+03</td>
<td align="right">1.80e+03</td>
<td align="right">2.50e+03</td>
<td align="right">4.00e+04</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">beds</td>
<td align="right">6608</td>
<td align="right">0.97</td>
<td align="right">1.89e+00</td>
<td align="right">1.08</td>
<td align="right">0.00e+00</td>
<td align="right">1.00e+00</td>
<td align="right">2.00e+00</td>
<td align="right">3.00e+00</td>
<td align="right">1.20e+01</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">baths</td>
<td align="right">158121</td>
<td align="right">0.21</td>
<td align="right">1.68e+00</td>
<td align="right">0.69</td>
<td align="right">1.00e+00</td>
<td align="right">1.00e+00</td>
<td align="right">2.00e+00</td>
<td align="right">2.00e+00</td>
<td align="right">8.00e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">sqft</td>
<td align="right">136117</td>
<td align="right">0.32</td>
<td align="right">1.20e+03</td>
<td align="right">5000.22</td>
<td align="right">8.00e+01</td>
<td align="right">7.50e+02</td>
<td align="right">1.00e+03</td>
<td align="right">1.36e+03</td>
<td align="right">9.00e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">room_in_apt</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">0.00e+00</td>
<td align="right">0.04</td>
<td align="right">0.00e+00</td>
<td align="right">0.00e+00</td>
<td align="right">0.00e+00</td>
<td align="right">0.00e+00</td>
<td align="right">1.00e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">lat</td>
<td align="right">193145</td>
<td align="right">0.04</td>
<td align="right">3.77e+01</td>
<td align="right">0.35</td>
<td align="right">3.36e+01</td>
<td align="right">3.74e+01</td>
<td align="right">3.78e+01</td>
<td align="right">3.78e+01</td>
<td align="right">4.04e+01</td>
<td align="left">▁▁▅▇▁</td>
</tr>
<tr class="odd">
<td align="left">lon</td>
<td align="right">196484</td>
<td align="right">0.02</td>
<td align="right">-1.22e+02</td>
<td align="right">0.78</td>
<td align="right">-1.23e+02</td>
<td align="right">-1.22e+02</td>
<td align="right">-1.22e+02</td>
<td align="right">-1.22e+02</td>
<td align="right">-7.42e+01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<pre class="r"><code>str(rent)</code></pre>
<pre><code>## spec_tbl_df [200,796 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ post_id    : chr [1:200796] &quot;pre2013_134138&quot; &quot;pre2013_135669&quot; &quot;pre2013_127127&quot; &quot;pre2013_68671&quot; ...
##  $ date       : num [1:200796] 20050111 20050126 20041017 20120601 20041021 ...
##  $ year       : num [1:200796] 2005 2005 2004 2012 2004 ...
##  $ nhood      : chr [1:200796] &quot;alameda&quot; &quot;alameda&quot; &quot;alameda&quot; &quot;alameda&quot; ...
##  $ city       : chr [1:200796] &quot;alameda&quot; &quot;alameda&quot; &quot;alameda&quot; &quot;alameda&quot; ...
##  $ county     : chr [1:200796] &quot;alameda&quot; &quot;alameda&quot; &quot;alameda&quot; &quot;alameda&quot; ...
##  $ price      : num [1:200796] 1250 1295 1100 1425 890 ...
##  $ beds       : num [1:200796] 2 2 2 1 1 1 1 3 NA 2 ...
##  $ baths      : num [1:200796] 2 NA NA NA NA NA 1 NA 1 NA ...
##  $ sqft       : num [1:200796] NA NA NA 735 NA NA NA NA NA NA ...
##  $ room_in_apt: num [1:200796] 0 0 0 0 0 0 0 0 0 0 ...
##  $ address    : chr [1:200796] NA NA NA NA ...
##  $ lat        : num [1:200796] NA NA NA NA NA NA NA NA NA NA ...
##  $ lon        : num [1:200796] NA NA NA NA NA NA NA NA NA NA ...
##  $ title      : chr [1:200796] &quot;$1250 / 2br - 2BR/2BA   1145 ALAMEDA DE LAS PULGAS&quot; &quot;$1295 / 2br - Walk the Beach! 1 FREE MONTH + $500 TRADER JOES SHOPPING CERTIFICATE&quot; &quot;$1100 / 2br - cottage&quot; &quot;$1425 / 1br - 735ft² - BEST LOCATION SOUTHSHORE GARDENS APARTMENTS&quot; ...
##  $ descr      : chr [1:200796] NA NA NA NA ...
##  $ details    : chr [1:200796] NA NA NA NA ...
##  - attr(*, &quot;spec&quot;)=
##   .. cols(
##   ..   post_id = col_character(),
##   ..   date = col_double(),
##   ..   year = col_double(),
##   ..   nhood = col_character(),
##   ..   city = col_character(),
##   ..   county = col_character(),
##   ..   price = col_double(),
##   ..   beds = col_double(),
##   ..   baths = col_double(),
##   ..   sqft = col_double(),
##   ..   room_in_apt = col_double(),
##   ..   address = col_character(),
##   ..   lat = col_double(),
##   ..   lon = col_double(),
##   ..   title = col_character(),
##   ..   descr = col_character(),
##   ..   details = col_character()
##   .. )
##  - attr(*, &quot;problems&quot;)=&lt;externalptr&gt;</code></pre>
<pre class="r"><code>head(rent)</code></pre>
<pre><code>## # A tibble: 6 × 17
##   post_id          date  year nhood city  county price  beds baths  sqft room_…¹
##   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 pre2013_134138 2.01e7  2005 alam… alam… alame…  1250     2     2    NA       0
## 2 pre2013_135669 2.01e7  2005 alam… alam… alame…  1295     2    NA    NA       0
## 3 pre2013_127127 2.00e7  2004 alam… alam… alame…  1100     2    NA    NA       0
## 4 pre2013_68671  2.01e7  2012 alam… alam… alame…  1425     1    NA   735       0
## 5 pre2013_127580 2.00e7  2004 alam… alam… alame…   890     1    NA    NA       0
## 6 pre2013_152345 2.01e7  2006 alam… alam… alame…   825     1    NA    NA       0
## # … with 6 more variables: address &lt;chr&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, title &lt;chr&gt;,
## #   descr &lt;chr&gt;, details &lt;chr&gt;, and abbreviated variable name ¹​room_in_apt</code></pre>
<p>Most column types correspond with what they should be, but date is stored as a double ‘20050111’ instead of a date 2005-01-11. Other column types like number of bedrooms (beds) are a double where an integer suffices. We could change this to integer to use less storage space. Description has the most missing values, after which address and details follow with also 190000+ missing values.</p>
<p>We have plotted the top 20 cities in terms of % of rental listings.</p>
<pre class="r"><code># creating a dataset with the top 20 cities by number of listings
top_20_cities &lt;- rent %&gt;%
  group_by(city) %&gt;%
  filter(year &lt; 2019) %&gt;% # ensuring we have the right years
  summarize(total_listings = n()) %&gt;%
  mutate(percentage = total_listings/sum(total_listings), # changing number to a percentage
         city = fct_reorder(city, total_listings)) %&gt;% #ordering by # listings
  slice_max(total_listings, n=20) 

# graphing the top 20 cities as a barplot
ggplot(top_20_cities) +
  aes(x = percentage, y = city) +
  geom_col() +
  labs(
    title = &#39;San Fransisco accounts for more than a quarter of all rental classifieds&#39;,
    subtitle = &#39;% of Craigslist listings, 2000-2018&#39;,
    x = NULL,
    y = NULL,
    caption = &#39;Source: Pennington, Kate(2018). Bay Area Craigslist Rental Housing Posts, 2000-2018&#39;,
    xticks = &#39;percentage&#39;
  ) +
  scale_x_continuous(labels = scales::percent) +
  theme_minimal(base_size=16)</code></pre>
<p><img src="/blogs/homework1_files/figure-html/top_cities-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>It is clear that San Francisco is responsible for 25% of the listings, making that the most interesting city to start investigating. To analyse what is happening with the rental prices in San Francisco, we have plotted the evolution of median prices in San Francisco for 0, 1, 2 and 3 bedroom listings.</p>
<pre class="r"><code># YOUR CODE GOES HERE

sf_rentals &lt;- rent %&gt;%
  filter(city == &#39;san francisco&#39;,
         beds &lt; 4) %&gt;% 
  mutate(beds = factor(beds)) %&gt;%
  group_by(beds, year) %&gt;%
  summarise(rent = median(price))
  

ggplot(sf_rentals) +
 aes(x = year, y = rent, color = beds) +
  facet_wrap(vars(beds), nrow = 1) +
  geom_line() +
  theme( legend.position = &#39;none&#39;) +
  labs(
    title = &#39;San Francisco rents have been steadily increasing&#39;,
    subtitle = &#39;0 to 3-bed listings, 2000-2018&#39;,
    x = NULL,
    y = NULL,
    caption = &#39;Source: Pennington, Kate(2018). Bay Area Craigslist Rental Housing Posts, 2000-2018&#39;,
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/sf_median_prices-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>We see that the rents have been increasing sincd 2005, with an exception for the recession in 2008. Since 2015 we see another decrease in rents for all sizes.</p>
<p>Having considered San Francisco, we turn to analyse the top 12 cities by number of listings in the Bay Area. We plot the median rental prices for those cities below.</p>
<pre class="r"><code># determining the top 12 cities in terms of listings
top12_cities &lt;- rent %&gt;%
  group_by(city) %&gt;%
  summarize(total_listings = n()) %&gt;%
  slice_max(total_listings, n=12)

# creating a vector with the city names
top12_cities &lt;- top12_cities$city

# gathering the dataset to plot, focusing on 1-bedroom flats
one_bed_bay_area &lt;- rent %&gt;%
  filter(beds == 1,
         city %in% top12_cities) %&gt;%
  group_by(city, year) %&gt;%
  summarize(rent = median(price))

# creating the plot
ggplot(one_bed_bay_area) +
  aes(x = year, y = rent, colour = city) +
  facet_wrap(vars(city)) +
  geom_line() +
  theme( legend.position = &#39;none&#39;) +
  labs(
    title = &#39;Rental prices for 1-bedroom flats in the Bay Area&#39;,
    x = NULL,
    y = NULL,
    caption = &#39;Source: Pennington, Kate(2018). Bay Area Craigslist Rental Housing Posts, 2000-2018&#39;,
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/spirit_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>We can clearly spot the financial crisis happening in 2008, when all the prices are going down across the cities and types of bedrooms in San Francisco. The effect is the smallest in Santa Rosa and Oakland, where we also don’t see an increase ahead of the recession, so a limited decrease is not unsurprising. The effect is the greatest where the increase is also the greatest, for example a three-bedroom in San Francisco or Santa Clara.</p>
</div>
<div id="analysis-of-movies--imdb-dataset" class="section level1">
<h1>Analysis of movies- IMDB dataset</h1>
<p>We are now analysing a dataset from imbd with 5000 movies. We will analyse the differences between genres in terms of ratings, popularity (facebook likes), and revenue.</p>
<pre class="r"><code>movies &lt;- read_csv(here::here(&#39;data&#39;,&quot;movies.csv&quot;))</code></pre>
<p>The movies dataset was imported and inspected using Skimr, and there were no missing values identified within the dataset. When checking for duplicates within the dataset, we identified that there were 54 duplicated titles which may result in duplicated entries within the dataset.</p>
<div id="table-showing-the-amount-of-imbd-movies-per-genre" class="section level3">
<h3>Table showing the amount of IMBD movies per genre</h3>
<pre class="r"><code>movies_by_genre &lt;- movies %&gt;% 
  group_by(genre) %&gt;% 
  summarize(count = n()) %&gt;% 
  arrange(desc(count))
  
movies_by_genre</code></pre>
<pre><code>## # A tibble: 17 × 2
##    genre       count
##    &lt;chr&gt;       &lt;int&gt;
##  1 Comedy        848
##  2 Action        738
##  3 Drama         498
##  4 Adventure     288
##  5 Crime         202
##  6 Biography     135
##  7 Horror        131
##  8 Animation      35
##  9 Fantasy        28
## 10 Documentary    25
## 11 Mystery        16
## 12 Sci-Fi          7
## 13 Family          3
## 14 Musical         2
## 15 Romance         2
## 16 Western         2
## 17 Thriller        1</code></pre>
</div>
<div id="table-showing-movie-revenue-indicators-per-genre" class="section level3">
<h3>Table showing movie revenue indicators per genre</h3>
<pre class="r"><code>movies %&gt;% 
  mutate(return_on_budget = gross/budget) %&gt;% 
  group_by(genre) %&gt;% 
  summarize(average_gross = mean(gross), average_budget = mean(budget),  average_return_on_budget = mean(return_on_budget)) %&gt;% 
  arrange(desc(average_return_on_budget)) </code></pre>
<pre><code>## # A tibble: 17 × 4
##    genre       average_gross average_budget average_return_on_budget
##    &lt;chr&gt;               &lt;dbl&gt;          &lt;dbl&gt;                    &lt;dbl&gt;
##  1 Horror          37713738.      13504916.                 88.3    
##  2 Biography       45201805.      28543696.                 22.3    
##  3 Musical         92084000        3189500                  18.8    
##  4 Family         149160478.      14833333.                 14.1    
##  5 Documentary     17353973.       5887852.                  8.70   
##  6 Western         20821884        3465000                   7.06   
##  7 Fantasy         42408841.      17582143.                  6.68   
##  8 Animation       98433792.      61701429.                  5.01   
##  9 Comedy          42630552.      24446319.                  3.71   
## 10 Mystery         67533021.      39218750                   3.27   
## 11 Romance         31264848.      25107500                   3.17   
## 12 Drama           37465371.      26242933.                  2.95   
## 13 Adventure       95794257.      66290069.                  2.41   
## 14 Crime           37502397.      26596169.                  2.17   
## 15 Action          86583860.      71354888.                  1.92   
## 16 Sci-Fi          29788371.      27607143.                  1.58   
## 17 Thriller            2468         300000                   0.00823</code></pre>
</div>
<div id="table-showing-the-top-15-directors-ranked-by-highest-mean-gross-revenue" class="section level3">
<h3>Table showing the Top 15 directors ranked by highest mean gross revenue</h3>
<pre class="r"><code>movies %&gt;% 
  group_by(director) %&gt;% 
  summarize(highest_gross_revenue = sum(gross), mean_gross_revenue = mean(gross), median_gross_revenue = median(gross), standard_deviation = sd(gross)) %&gt;% 
  slice_max(highest_gross_revenue, n = 15)</code></pre>
<pre><code>## # A tibble: 15 × 5
##    director          highest_gross_revenue mean_gross_revenue median_g…¹ stand…²
##    &lt;chr&gt;                             &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;
##  1 Steven Spielberg             4014061704         174524422. 164435221   1.01e8
##  2 Michael Bay                  2231242537         171634041. 138396624   1.27e8
##  3 Tim Burton                   2071275480         129454718.  76519172   1.09e8
##  4 Sam Raimi                    2014600898         201460090. 234903076   1.62e8
##  5 James Cameron                1909725910         318287652. 175562880.  3.09e8
##  6 Christopher Nolan            1813227576         226653447  196667606.  1.87e8
##  7 George Lucas                 1741418480         348283696  380262555   1.46e8
##  8 Robert Zemeckis              1619309108         124562239. 100853835   9.13e7
##  9 Clint Eastwood               1378321100          72543216.  46700000   7.55e7
## 10 Francis Lawrence             1358501971         271700394. 281666058   1.35e8
## 11 Ron Howard                   1335988092         111332341  101587923   8.19e7
## 12 Gore Verbinski               1329600995         189942999. 123207194   1.54e8
## 13 Andrew Adamson               1137446920         284361730  279680930.  1.21e8
## 14 Shawn Levy                   1129750988         102704635.  85463309   6.55e7
## 15 Ridley Scott                 1128857598          80632686.  47775715   6.88e7
## # … with abbreviated variable names ¹​median_gross_revenue, ²​standard_deviation</code></pre>
</div>
<div id="graphics-showing-the-spread-of-imdb-ratings-per-genre" class="section level2">
<h2>Graphics showing the spread of IMDB ratings per genre</h2>
<div id="table-showing-the-summary-statistics-for-imdb-movie-ratings" class="section level3">
<h3>Table showing the summary statistics for IMDB movie ratings</h3>
<p>We have added both a density graph and a box plot to visually represent how ratings are distributed. We believe that the box plot more accurately represents the distribution of ratings per genre, with taking the count of rating submissions as well. When cross-analyzing the various distributions, the box plot diagrams allow for an easier comparison between genres.</p>
<pre class="r"><code># summarizing the dataset by genre
data &lt;- movies %&gt;% 
  group_by(genre) %&gt;% 
  summarize(mean = mean(rating), min = min(rating), max(rating), median = median(rating), standard_dev = sd(rating))

data</code></pre>
<pre><code>## # A tibble: 17 × 6
##    genre        mean   min `max(rating)` median standard_dev
##    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;
##  1 Action       6.23   2.1           9     6.3         1.03 
##  2 Adventure    6.51   2.3           8.6   6.6         1.09 
##  3 Animation    6.65   4.5           8     6.9         0.968
##  4 Biography    7.11   4.5           8.9   7.2         0.760
##  5 Comedy       6.11   1.9           8.8   6.2         1.02 
##  6 Crime        6.92   4.8           9.3   6.9         0.849
##  7 Documentary  6.66   1.6           8.5   7.4         1.77 
##  8 Drama        6.73   2.1           8.8   6.8         0.917
##  9 Family       6.5    5.7           7.9   5.9         1.22 
## 10 Fantasy      6.15   4.3           7.9   6.45        0.959
## 11 Horror       5.83   3.6           8.5   5.9         1.01 
## 12 Musical      6.75   6.3           7.2   6.75        0.636
## 13 Mystery      6.86   4.6           8.5   6.9         0.882
## 14 Romance      6.65   6.2           7.1   6.65        0.636
## 15 Sci-Fi       6.66   5             8.2   6.4         1.09 
## 16 Thriller     4.8    4.8           4.8   4.8        NA    
## 17 Western      5.7    4.1           7.3   5.7         2.26</code></pre>
<pre class="r"><code>#We have added an additional visualisation for the representation of the distribution of ratings
ggplot(movies) + aes(x = rating, y = genre ) + geom_boxplot() + labs(
    title = &#39;Distribution of IMDb movie ratings is largely uniform accross genres&#39;,
    subtitle = &#39;Box plot showing the variation of IMDB genre ratings&#39;,
    x = &#39;Ratings&#39;,
    y = &#39;Genres&#39;,
    caption = &#39;Source: Kaggle IMDB 5000 movie dataset&#39;,
    xticks = &#39;&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/ratings-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Required visualisation
ggplot(movies) + aes(rating) + geom_density() +
  facet_wrap(vars(genre)) + labs(
    title = &#39;Distribution of IMDb movie ratings is largely uniform accross genres&#39;,
    subtitle = &#39;Density graph showing the variation in IMDB genre ratings&#39;,
    x = &#39;Ratings&#39;,
    y = &#39;Density&#39;,
    caption = &#39;Source: Kaggle IMDB 5000 movie dataset&#39;,
    xticks = &#39;&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/ratings-2.png" width="648" style="display: block; margin: auto;" /></p>
<!-- ### Use `ggplot` to answer the following

-   Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes? -->
<blockquote>
<p>The number of facebook likes is not a good predictor of how much money a movie will make at the box office as movies with the same number of likes received by cast earned vastly different amounts.</p>
</blockquote>
<pre class="r"><code># creating a scatterplot of the likes and revenue
ggplot(movies) +
  aes(x = cast_facebook_likes, y = gross) +
  geom_point() +
  scale_x_log10(labels = comma) +
  scale_y_continuous(labels = dollar) + #We changed the scale of the visualsation to make it more presentable
  labs(
    title = &quot;Number of Cast&#39;s Facebook Likes is not a Good Predictor of Gross Movie Earnings&quot;,
    subtitle = &#39;Relationship Between Facebook Likes Received by Cast and Total US Earnings&#39;,
    x = &quot;# Facebook Likes Cast Members Received&quot;,
    y = &quot;Gross Earnings in the US Box Office, not Inflation-adjusted&quot;,
    caption = &#39;Source: Kaggle IMDB 5000 movie dataset&#39;)</code></pre>
<p><img src="/blogs/homework1_files/figure-html/gross_on_fblikes-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Gross US Earnings could be predicted by looking at the movie’s budget as the two variables display a positive relationship relationship.</p>
<pre class="r"><code># creating a scatterplot to analyse the revenue and ratings
ggplot(movies) +
  aes(x = budget, y = gross) +
  geom_point() +
  scale_x_continuous(labels = dollar) + 
  scale_y_continuous(labels = dollar) +
  geom_smooth(method=&#39;lm&#39;) + #We added a line of best fit to the visualsation to plot the general trend within the revenue
    labs(
    title = &quot;Gross US Earnings Can Be Consistently Predicted by the Movie&#39;s Budget&quot;,
    subtitle = &quot;Relationship Between Movie&#39;s Budget by Cast and Total US Earnings&quot;,
    x = &quot;Movie&#39;s Budget&quot;,
    y = &quot;Gross Earnings in the US Box Office, not Inflation-adjusted&quot;,
    caption = &#39;Source: Kaggle IMDB 5000 movie dataset&#39;)</code></pre>
<p><img src="/blogs/homework1_files/figure-html/gross_on_budget-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>In general, IMDB ratings could be used to predict the gross earnings of movies in the US - this positive relationship is particularly visible in the Action, Adventure, and Comedy genres. However, data in the ‘movies’ dataset is not uniformly distributed across genres, with some genres containing only a few data points, which prohibits a meaningful analysis of the relationship between IMDB ratings and gross earnings.</p>
<p>Outliers are most visible in the Action, Drama, and Family genres. Some genres (Biography, Crime) only contain movies with a minimum rating of approximately 5.0 - it might be the case that movies of these types generally receive higher ratings. Lastly, genres such as Fantasy and Sci-Fi do not exhibit any relationship between IMDB ratings and gross earnings.</p>
<pre class="r"><code># Creating a scatterplot of revenue and rating, faceted by genre
ggplot(movies) +
  aes(x = rating, y = gross, color = genre) +
  facet_wrap(vars(genre)) +
  theme(legend.position = &#39;none&#39;) +
  geom_point() +
  scale_y_continuous(labels = dollar) +
  labs(
    title = &quot;Gross earnings could be predicted with IMDB ratings, with some genres lacking enough data points to identify a positive relationship&quot;,
    subtitle = &#39;Relationship Between IMDB Ratings and Total US Earnings&#39;,
    x = &quot;IMDB Rating&quot;,
    y = &quot;Gross Earnings in the US Box Office, not Inflation-adjusted&quot;,
    caption = &#39;Source: Kaggle IMDB 5000 movie dataset&#39;,
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/gross_on_rating-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="returns-of-financial-stocks" class="section level1">
<h1>Returns of financial stocks</h1>
<p>Before we can analyse the returns of stocks, we decide which companies we want to analyse.</p>
<pre class="r"><code>nyse &lt;- read_csv(here::here(&quot;data&quot;,&quot;nyse.csv&quot;))
glimpse(nyse)</code></pre>
<pre><code>## Rows: 508
## Columns: 6
## $ symbol        &lt;chr&gt; &quot;MMM&quot;, &quot;ABB&quot;, &quot;ABT&quot;, &quot;ABBV&quot;, &quot;ACN&quot;, &quot;AAP&quot;, &quot;AFL&quot;, &quot;A&quot;, &quot;…
## $ name          &lt;chr&gt; &quot;3M Company&quot;, &quot;ABB Ltd&quot;, &quot;Abbott Laboratories&quot;, &quot;AbbVie …
## $ ipo_year      &lt;chr&gt; &quot;n/a&quot;, &quot;n/a&quot;, &quot;n/a&quot;, &quot;2012&quot;, &quot;2001&quot;, &quot;n/a&quot;, &quot;n/a&quot;, &quot;1999…
## $ sector        &lt;chr&gt; &quot;Health Care&quot;, &quot;Consumer Durables&quot;, &quot;Health Care&quot;, &quot;Heal…
## $ industry      &lt;chr&gt; &quot;Medical/Dental Instruments&quot;, &quot;Electrical Products&quot;, &quot;Ma…
## $ summary_quote &lt;chr&gt; &quot;https://www.nasdaq.com/symbol/mmm&quot;, &quot;https://www.nasdaq…</code></pre>
<p>Based on this dataset, we are showing the number of companies per sector.</p>
<pre class="r"><code>comp_per_sector &lt;- nyse %&gt;% 
  group_by(sector) %&gt;% 
  summarise(companies = n()) %&gt;% 
  slice_max(companies, n=100) %&gt;% 
  mutate(sector = fct_reorder(sector, companies))
comp_per_sector </code></pre>
<pre><code>## # A tibble: 12 × 2
##    sector                companies
##    &lt;fct&gt;                     &lt;int&gt;
##  1 Finance                      97
##  2 Consumer Services            79
##  3 Public Utilities             60
##  4 Capital Goods                45
##  5 Health Care                  45
##  6 Energy                       42
##  7 Technology                   40
##  8 Basic Industries             39
##  9 Consumer Non-Durables        31
## 10 Miscellaneous                12
## 11 Transportation               10
## 12 Consumer Durables             8</code></pre>
<pre class="r"><code>ggplot(comp_per_sector) +
  aes(x = companies, y = sector) +
  geom_col() +
  labs(
    title = &#39;Finance is the Largest Sector, Consumer Durables is the smallest&#39;,
    subtitle = &quot;Companies Per Sector&quot;,
    x = &#39;Companies&#39;,
    y = &#39;Sector&#39;,
    caption = &#39;Source: Federal Reserve Economic Database&#39;,
  ) </code></pre>
<p><img src="/blogs/homework1_files/figure-html/companies_per_sector-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>comp_per_sector &lt;- nyse %&gt;% 
  group_by(sector) %&gt;% 
  summarise(companies = n()) %&gt;% 
  slice_max(companies, n=100) %&gt;% 
  mutate(sector = fct_reorder(sector, companies))
  
ggplot(comp_per_sector) +
  aes(x = companies, y = sector) +
  geom_col()</code></pre>
<p><img src="/blogs/homework1_files/figure-html/companies_per_sector-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>comp_per_sector</code></pre>
<pre><code>## # A tibble: 12 × 2
##    sector                companies
##    &lt;fct&gt;                     &lt;int&gt;
##  1 Finance                      97
##  2 Consumer Services            79
##  3 Public Utilities             60
##  4 Capital Goods                45
##  5 Health Care                  45
##  6 Energy                       42
##  7 Technology                   40
##  8 Basic Industries             39
##  9 Consumer Non-Durables        31
## 10 Miscellaneous                12
## 11 Transportation               10
## 12 Consumer Durables             8</code></pre>
<pre class="r"><code>nyse</code></pre>
<pre><code>## # A tibble: 508 × 6
##    symbol name                             ipo_year sector       indus…¹ summa…²
##    &lt;chr&gt;  &lt;chr&gt;                            &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;  
##  1 MMM    3M Company                       n/a      Health Care  Medica… https:…
##  2 ABB    ABB Ltd                          n/a      Consumer Du… Electr… https:…
##  3 ABT    Abbott Laboratories              n/a      Health Care  Major … https:…
##  4 ABBV   AbbVie Inc.                      2012     Health Care  Major … https:…
##  5 ACN    Accenture plc                    2001     Miscellaneo… Busine… https:…
##  6 AAP    Advance Auto Parts Inc           n/a      Consumer Se… Other … https:…
##  7 AFL    Aflac Incorporated               n/a      Finance      Accide… https:…
##  8 A      Agilent Technologies, Inc.       1999     Capital Goo… Biotec… https:…
##  9 AEM    Agnico Eagle Mines Limited       n/a      Basic Indus… Precio… https:…
## 10 APD    Air Products and Chemicals, Inc. n/a      Basic Indus… Major … https:…
## # … with 498 more rows, and abbreviated variable names ¹​industry,
## #   ²​summary_quote</code></pre>
<p>We are choosing some stocks and downloading their ticker data to analyse.</p>
<pre class="r"><code># Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, 
# cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks &lt;- c(&quot;MMM&quot;,&quot;ABT&quot;,&quot;ACN&quot;,&quot;ANTM&quot;,&quot;AGR&quot;,&quot;TSLA&quot;,&quot;SPY&quot; ) %&gt;%
  tq_get(get  = &quot;stock.prices&quot;,
         from = &quot;2011-01-01&quot;,
         to   = &quot;2022-08-31&quot;) %&gt;%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame</code></pre>
<pre><code>## Rows: 16,362
## Columns: 8
## Groups: symbol [6]
## $ symbol   &lt;chr&gt; &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;…
## $ date     &lt;date&gt; 2011-01-03, 2011-01-04, 2011-01-05, 2011-01-06, 2011-01-07, …
## $ open     &lt;dbl&gt; 86.8, 87.0, 86.3, 86.9, 86.6, 85.7, 87.3, 88.0, 88.5, 87.7, 8…
## $ high     &lt;dbl&gt; 87.3, 87.3, 87.9, 87.2, 87.3, 87.3, 88.3, 88.8, 88.9, 88.1, 8…
## $ low      &lt;dbl&gt; 86.7, 86.3, 86.1, 85.6, 85.9, 85.7, 87.3, 87.9, 87.8, 87.4, 8…
## $ close    &lt;dbl&gt; 86.8, 86.7, 86.7, 86.1, 86.2, 87.2, 87.7, 88.7, 88.0, 88.1, 8…
## $ volume   &lt;dbl&gt; 2632800, 2644100, 4081300, 3452600, 3355500, 3475200, 3024400…
## $ adjusted &lt;dbl&gt; 62.3, 62.2, 62.2, 61.9, 61.9, 62.6, 63.0, 63.7, 63.2, 63.3, 6…</code></pre>
<p>Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.</p>
<pre class="r"><code>#calculate daily returns
myStocks_returns_daily &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;daily&quot;, 
               type       = &quot;log&quot;,
               col_rename = &quot;daily_returns&quot;,
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;monthly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;monthly_returns&quot;,
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual &lt;- myStocks %&gt;%
  group_by(symbol) %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;yearly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;yearly_returns&quot;,
               cols = c(nested.col))</code></pre>
<div id="summary-statistics-for-monthly-returns" class="section level3">
<h3>Summary Statistics For Monthly Returns</h3>
<pre class="r"><code>summary_monthly_returns &lt;- myStocks_returns_monthly %&gt;% 
  
  group_by(symbol) %&gt;% 
  summarise(min_return = min(monthly_returns),
            max_return = max(monthly_returns),
            median_return = median(monthly_returns),
            mean_return = mean(monthly_returns),
            sd_return = sd(monthly_returns))
  
  
   
summary_monthly_returns</code></pre>
<pre><code>## # A tibble: 6 × 6
##   symbol min_return max_return median_return mean_return sd_return
##   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;
## 1 ABT        -0.152      0.172       0.0116      0.0140     0.0539
## 2 ACN        -0.145      0.160       0.0216      0.0164     0.0613
## 3 AGR        -0.112      0.186       0.00272     0.00838    0.0521
## 4 MMM        -0.150      0.113       0.0121      0.00651    0.0551
## 5 SPY        -0.125      0.127       0.0146      0.0106     0.0404
## 6 TSLA       -0.224      0.811       0.0117      0.0501     0.177</code></pre>
<p>To analyse the distribution of the returns, we plot a density plot for each of the stocks.</p>
<pre class="r"><code>myStocks_returns_monthly %&gt;% 
  group_by(symbol) %&gt;% 
  ggplot() +
  aes (x = monthly_returns, fill = symbol) +
  geom_density() +
  labs(
    title = &#39;TSLA Has the Widest Spread&#39;,
    subtitle = &#39;Density Plot - Stockwise Monthly Return&#39;,
    x = &#39;Return&#39;,
    y = &#39;Density&#39;,
    caption = &#39;Source: Federal Reserve Economic Database&#39;
  ) +
  facet_wrap(~symbol, ncol = 1) +
  #facet_grid()+
 theme_bw()+
  theme(legend.position = &quot;none&quot;)+
  scale_x_continuous(labels = scales::percent)</code></pre>
<p><img src="/blogs/homework1_files/figure-html/density_monthly_returns-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># YOUR CODE GOES HERE</code></pre>
<p>The graph shows that TSLA has the widest spread, suggesting it has the greatest deviation from the mean and thus is the most risky stock. The remaining stocks have a similar spread, we need further analysis to come to a concrete conclusion to determine which stock is the least risky.</p>
<p>Finally we made a plot to show the expected monthly return per stock.</p>
<pre class="r"><code>  summary_monthly_returns %&gt;% 
  ggplot() +
  aes(x = mean_return, y = sd_return, label = symbol, colour = symbol)+
  ggrepel::geom_text_repel()+
  geom_point()+
  coord_flip()+
 # theme_minimal()+
  theme(legend.position = &quot;none&quot;)+
  scale_x_continuous(labels = scales::percent)+
  scale_y_continuous(labels = scales::percent)+
  labs(
    title = &quot;Tesla Stock Involves the Highest Risk and Return&quot; ,
    subtitle = &quot;Risk Return Analysis&quot;,
    x = &quot;Percentage Risk Involved&quot;,
    y = &quot;Expected Monthly Return&quot;,
    caption = &#39;Source: Federal Reserve Economic Database&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/risk_return_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>According to our analysis, TSLA stock is the most risky stock with fluctuations around 5%. It also offers the highest return with an expected monthly return of 16%. On the other hand MMM provides the least risk with fluctations less than 1% and also offers a relatively high retrun, which is above 4%.</p>
</div>
</div>
<div id="on-your-own-spotify" class="section level1">
<h1>On your own: Spotify</h1>
<p>We have downloaded a large dataset on spotify songs and are going to analyse what makes a track popular.</p>
<pre class="r"><code>spotify_songs &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&#39;)</code></pre>
<p>The data dictionary can be found below</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>variable</strong></th>
<th><strong>class</strong></th>
<th><strong>description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>track_id</td>
<td>character</td>
<td>Song unique ID</td>
</tr>
<tr class="even">
<td>track_name</td>
<td>character</td>
<td>Song Name</td>
</tr>
<tr class="odd">
<td>track_artist</td>
<td>character</td>
<td>Song Artist</td>
</tr>
<tr class="even">
<td>track_popularity</td>
<td>double</td>
<td>Song Popularity (0-100) where higher is better</td>
</tr>
<tr class="odd">
<td>track_album_id</td>
<td>character</td>
<td>Album unique ID</td>
</tr>
<tr class="even">
<td>track_album_name</td>
<td>character</td>
<td>Song album name</td>
</tr>
<tr class="odd">
<td>track_album_release_date</td>
<td>character</td>
<td>Date when album released</td>
</tr>
<tr class="even">
<td>playlist_name</td>
<td>character</td>
<td>Name of playlist</td>
</tr>
<tr class="odd">
<td>playlist_id</td>
<td>character</td>
<td>Playlist ID</td>
</tr>
<tr class="even">
<td>playlist_genre</td>
<td>character</td>
<td>Playlist genre</td>
</tr>
<tr class="odd">
<td>playlist_subgenre</td>
<td>character</td>
<td>Playlist subgenre</td>
</tr>
<tr class="even">
<td>danceability</td>
<td>double</td>
<td>Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.</td>
</tr>
<tr class="odd">
<td>energy</td>
<td>double</td>
<td>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.</td>
</tr>
<tr class="even">
<td>key</td>
<td>double</td>
<td>The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.</td>
</tr>
<tr class="odd">
<td>loudness</td>
<td>double</td>
<td>The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.</td>
</tr>
<tr class="even">
<td>mode</td>
<td>double</td>
<td>Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.</td>
</tr>
<tr class="odd">
<td>speechiness</td>
<td>double</td>
<td>Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.</td>
</tr>
<tr class="even">
<td>acousticness</td>
<td>double</td>
<td>A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.</td>
</tr>
<tr class="odd">
<td>instrumentalness</td>
<td>double</td>
<td>Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.</td>
</tr>
<tr class="even">
<td>liveness</td>
<td>double</td>
<td>Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.</td>
</tr>
<tr class="odd">
<td>valence</td>
<td>double</td>
<td>A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).</td>
</tr>
<tr class="even">
<td>tempo</td>
<td>double</td>
<td>The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.</td>
</tr>
<tr class="odd">
<td>duration_ms</td>
<td>double</td>
<td>Duration of song in milliseconds</td>
</tr>
</tbody>
</table>
<p>Before we can get into analysing this dataset, we need to examine the distribution of the popularity of tracks on spotify. See below the histogram that shows the distribution.</p>
<pre class="r"><code>ggplot(spotify_songs) +
  aes(x = track_popularity) +
  geom_histogram() +
  labs(
    title = &#39;Many songs have a popularity of 0 on spotify&#39;,
    subtitle = &#39;A histogram of the popularity rating of spotify songs&#39;,
    x = &#39;Popularity rating&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-2-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>The track popularity seems to be a roughly normal distribution, with a lot of tracks with close to 0 popularity that do not fit with the distribution. This can be explained by a lot of small artists posting their own songs that not many people listen to, like someone creating a new podcast in their garage. Like in the music industry in general, not everyone makes it.</p>
<p>To start determining why some songs are more popular than others, we first show the distribution and summary statistics of the variables. We will try to see if we can determine whether the features look like a normal distribution.</p>
<pre class="r"><code>skim(spotify_songs) # seeing if we can determine whether the audio features look like a normal distribution</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-3">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">spotify_songs</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">32833</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">23</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">10</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">13</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table style="width:100%;">
<colgroup>
<col width="30%" />
<col width="12%" />
<col width="16%" />
<col width="4%" />
<col width="4%" />
<col width="7%" />
<col width="10%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">track_id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22</td>
<td align="right">22</td>
<td align="right">0</td>
<td align="right">28356</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">track_name</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">144</td>
<td align="right">0</td>
<td align="right">23449</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">track_artist</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">69</td>
<td align="right">0</td>
<td align="right">10692</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">track_album_id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22</td>
<td align="right">22</td>
<td align="right">0</td>
<td align="right">22545</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">track_album_name</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">151</td>
<td align="right">0</td>
<td align="right">19743</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">track_album_release_date</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">4530</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">playlist_name</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6</td>
<td align="right">120</td>
<td align="right">0</td>
<td align="right">449</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">playlist_id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22</td>
<td align="right">22</td>
<td align="right">0</td>
<td align="right">471</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">playlist_genre</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">playlist_subgenre</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">25</td>
<td align="right">0</td>
<td align="right">24</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="15%" />
<col width="8%" />
<col width="12%" />
<col width="8%" />
<col width="8%" />
<col width="6%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">track_popularity</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">42.48</td>
<td align="right">24.98</td>
<td align="right">0.0</td>
<td align="right">24.00</td>
<td align="right">45.00</td>
<td align="right">62.00</td>
<td align="right">1.00e+02</td>
<td align="left">▆▆▇▆▁</td>
</tr>
<tr class="even">
<td align="left">danceability</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.65</td>
<td align="right">0.15</td>
<td align="right">0.0</td>
<td align="right">0.56</td>
<td align="right">0.67</td>
<td align="right">0.76</td>
<td align="right">9.80e-01</td>
<td align="left">▁▁▃▇▃</td>
</tr>
<tr class="odd">
<td align="left">energy</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.70</td>
<td align="right">0.18</td>
<td align="right">0.0</td>
<td align="right">0.58</td>
<td align="right">0.72</td>
<td align="right">0.84</td>
<td align="right">1.00e+00</td>
<td align="left">▁▁▅▇▇</td>
</tr>
<tr class="even">
<td align="left">key</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5.37</td>
<td align="right">3.61</td>
<td align="right">0.0</td>
<td align="right">2.00</td>
<td align="right">6.00</td>
<td align="right">9.00</td>
<td align="right">1.10e+01</td>
<td align="left">▇▂▅▅▆</td>
</tr>
<tr class="odd">
<td align="left">loudness</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-6.72</td>
<td align="right">2.99</td>
<td align="right">-46.5</td>
<td align="right">-8.17</td>
<td align="right">-6.17</td>
<td align="right">-4.64</td>
<td align="right">1.27e+00</td>
<td align="left">▁▁▁▂▇</td>
</tr>
<tr class="even">
<td align="left">mode</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.57</td>
<td align="right">0.50</td>
<td align="right">0.0</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00e+00</td>
<td align="left">▆▁▁▁▇</td>
</tr>
<tr class="odd">
<td align="left">speechiness</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.11</td>
<td align="right">0.10</td>
<td align="right">0.0</td>
<td align="right">0.04</td>
<td align="right">0.06</td>
<td align="right">0.13</td>
<td align="right">9.20e-01</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="even">
<td align="left">acousticness</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.18</td>
<td align="right">0.22</td>
<td align="right">0.0</td>
<td align="right">0.02</td>
<td align="right">0.08</td>
<td align="right">0.26</td>
<td align="right">9.90e-01</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">instrumentalness</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.08</td>
<td align="right">0.22</td>
<td align="right">0.0</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">9.90e-01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">liveness</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.19</td>
<td align="right">0.15</td>
<td align="right">0.0</td>
<td align="right">0.09</td>
<td align="right">0.13</td>
<td align="right">0.25</td>
<td align="right">1.00e+00</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">valence</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.51</td>
<td align="right">0.23</td>
<td align="right">0.0</td>
<td align="right">0.33</td>
<td align="right">0.51</td>
<td align="right">0.69</td>
<td align="right">9.90e-01</td>
<td align="left">▃▇▇▇▃</td>
</tr>
<tr class="even">
<td align="left">tempo</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">120.88</td>
<td align="right">26.90</td>
<td align="right">0.0</td>
<td align="right">99.96</td>
<td align="right">121.98</td>
<td align="right">133.92</td>
<td align="right">2.39e+02</td>
<td align="left">▁▂▇▂▁</td>
</tr>
<tr class="odd">
<td align="left">duration_ms</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">225799.81</td>
<td align="right">59834.01</td>
<td align="right">4000.0</td>
<td align="right">187819.00</td>
<td align="right">216000.00</td>
<td align="right">253585.00</td>
<td align="right">5.18e+05</td>
<td align="left">▁▇▇▁▁</td>
</tr>
</tbody>
</table>
<pre class="r"><code>ggplot(spotify_songs) +
  aes(x = acousticness) +
  geom_histogram() +
  labs(
    title = &#39;Acousticness is exponentially distributed&#39;,
    subtitle = &#39;Distribution of acousticness&#39;,
    x = &#39;Acousticness&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># acousticness looks like an exponential distribution, with most songs not having any acousticness at all.

ggplot(spotify_songs) +
  aes(x = liveness) +
  geom_histogram() +
  labs(
    title = &#39;Liveness is skewed right&#39;,
    subtitle = &#39;Distribution of Liveness&#39;,
    x = &#39;Liveness&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Liveness is skewed right. Most songs have some liveness, but a few have a lot.

ggplot(spotify_songs) +
  aes(x = speechiness) +
  geom_histogram() +
  labs(
    title = &#39;Speechiness is exponentially distributed&#39;,
    subtitle = &#39;Distribution of Speechiness&#39;,
    x = &#39;Speechiness&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-3.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Speechiness is an exponential distribution, similar to acousticness. Speechiness, however, has very few songs with exactly 0 speechiness.

ggplot(spotify_songs) +
  aes(x = instrumentalness) +
  geom_histogram() +
  labs(
    title = &#39;Instrumentalness is skewed right&#39;,
    subtitle = &#39;Distribution of Instrumentalness&#39;,
    x = &#39;Instrumentalness&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-4.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Instrumentalness is skewed right. The vast majority of songs have a 0 instrumentalness, but there&#39;s another small bump around 0.8 where some songs have a lot of instrumentalness.

ggplot(spotify_songs) +
  aes(x = energy) +
  geom_histogram() +
  labs(
    title = &#39;Energy is normally distributed with a cutoff&#39;,
    subtitle = &#39;Distribution of Energy&#39;,
    x = &#39;Energy&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-5.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Energy looks like a normal distribution, with a mean around 0.75. However, it is cut off at 1 so it is not a normal distribution.

ggplot(spotify_songs) +
  aes(x = loudness) +
  geom_histogram() +
  labs(
    title = &#39;Loudness is normally distributed with a small standard deviation&#39;,
    subtitle = &#39;Distribution of Loudness&#39;,
    x = &#39;Loudness&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-6.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Loudness looks like a normal distribution with a very small standard deviation, though it is slightly skewed left. 

ggplot(spotify_songs) +
  aes(x = danceability) +
  geom_histogram() +
  labs(
    title = &#39;Danceability is normally distributed&#39;,
    subtitle = &#39;Distribution of Danceability&#39;,
    x = &#39;Danceability&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-7.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Danceability looks like a normal distribution, slightly skewed left.

ggplot(spotify_songs) +
  aes(x = valence) +
  geom_histogram() +
  labs(
    title = &#39;Valence is normally distributed with a cutoff&#39;,
    subtitle = &#39;Distribution of Valence&#39;,
    x = &#39;Valence&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-8.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Valence looks like a normal distribution that&#39;s cut off at 0 and 1.

ggplot(spotify_songs) +
  aes(x = tempo) +
  geom_histogram()  +
  labs(
    title = &#39;Tempo  is bimodal normally distributed&#39;,
    subtitle = &#39;Distribution of Tempo&#39;,
    x = &#39;Tempo&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-9.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Tempo looks like a bimodal noraml distribution, with the 2 peaks being 2 means. 

ggplot(spotify_songs) +
  aes(x = duration_ms) +
  geom_histogram()  +
  labs(
    title = &#39;Duration is normally distributed, skewed to the right&#39;,
    subtitle = &#39;Distribution of Duration&#39;,
    x = &#39;Duration&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-10.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Duration appears as a normal distribution with a skew to the right.

ggplot(spotify_songs) +
  aes(x = key) +
  geom_histogram()  +
  labs(
    title = &#39;Key can only be whole numbers&#39;,
    subtitle = &#39;Distribution of Key&#39;,
    x = &#39;Key&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-11.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># The key does not have values everywhere on the scale, it looks like only whole numbers are possible on the key. 

ggplot(spotify_songs) +
  aes(x = mode) +
  geom_histogram() +
  labs(
    title = &#39;Mode is either 1 (Major) or  0 (Minor)&#39;,
    subtitle = &#39;Distribution of Mode&#39;,
    x = &#39;Mode&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-3-12.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#  Mode only includes 0 or 1, nothing else.</code></pre>
<p>We choose valence and danceability to start analysing what determines the track popularity.</p>
<pre class="r"><code>ggplot(spotify_songs) +
  aes(valence, track_popularity) +
  geom_point() +
  geom_smooth() +
  labs(
    title = &#39;Valence and track popularity appear unrelated&#39;,
    subtitle = &#39;Scatterplot of Valence and popularity&#39;,
    x = &#39;Valence&#39;,
    y = &#39;Popularity&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-4-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>cor(spotify_songs$valence, spotify_songs$track_popularity)</code></pre>
<pre><code>## [1] 0.0332</code></pre>
<p>Track popularity and valence do not appear to have a clear relationship, the values for both are spread out across the scatter plot and the correlation coefficient is very low.</p>
<pre class="r"><code>ggplot(spotify_songs) +
  aes(danceability, track_popularity) +
  geom_point() +
  geom_smooth() +
  labs(
    title = &#39;High danceability is related with a higher popularity&#39;,
    subtitle = &#39;Scatterplot of danceability and popularity&#39;,
    x = &#39;Danceability&#39;,
    y = &#39;Popularity&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-5-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>cor(spotify_songs$danceability, spotify_songs$track_popularity)</code></pre>
<pre><code>## [1] 0.0647</code></pre>
<p>The correlation coefficient is again very low, but a much higher danceability is associated with a slightly higher popularity. It seems that both of these tracks do not do much to predict the popularity of songs.</p>
<p>Next we determine whether the modality matters for the danceability and track popularity.</p>
<pre class="r"><code>mode_analysis &lt;- spotify_songs %&gt;%
  group_by(mode) %&gt;%
  summarize(avg_danceability = mean(danceability),
            median_danceability = median(danceability),
            avg_popularity = mean(track_popularity),
            median_popularity = median(track_popularity))

mode_analysis</code></pre>
<pre><code>## # A tibble: 2 × 5
##    mode avg_danceability median_danceability avg_popularity median_popularity
##   &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;
## 1     0            0.665               0.68            42.2                45
## 2     1            0.647               0.663           42.7                46</code></pre>
<pre class="r"><code>library(BSDA)

spotify_songs %&gt;%
  mutate(mode = factor(mode)) %&gt;%
ggplot() +
  aes(x = danceability, fill = mode) +
  geom_histogram(alpha = 0.2, position = &#39;identity&#39;) +
  labs(
    title = &#39;Danceability does not differ between major and minor&#39;,
    subtitle = &#39;Histograms of danceability for major (1) and minor (0) songs&#39;,
    x = &#39;Danceability&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-7-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mode1 &lt;- filter(spotify_songs, mode == 1)
mode0 &lt;- filter(spotify_songs, mode == 0)</code></pre>
<p>There appears to be no significant difference in danceability between major and minor modes, as the mean appears in the same in both graphs and the spread does not appear different.</p>
<pre class="r"><code>spotify_songs %&gt;%
  mutate(mode = factor(mode)) %&gt;%
ggplot() +
  aes(x = track_popularity, fill = mode) +
  geom_histogram(alpha = 0.2, position = &#39;identity&#39;) +
  labs(
    title = &#39;Track popularity does not differ between major and minor&#39;,
    subtitle = &#39;Histograms of Track popularity for major (1) and minor (0) songs&#39;,
    x = &#39;Track popularity&#39;,
    y = &#39;Count&#39;, 
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-8-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mode1 &lt;- filter(spotify_songs, mode == 1)
mode0 &lt;- filter(spotify_songs, mode == 0)</code></pre>
<p>There appears to be no significant difference in track_popularity between major and minor modes. Based on these analyses, we cannot yet decide what audio features determine the popularity of songs.</p>
</div>
<div id="challenge-1-replicating-a-chart" class="section level1">
<h1>Challenge 1: Replicating a chart</h1>
<p>We have created a graph to show the cumulative % change in median rental prices for 0-, 1-, and 2-bed flats between 2000 and 2018 for the top twelve cities in Bay Area.</p>
<pre class="r"><code>base_years &lt;- rent %&gt;%
  group_by(beds, city) %&gt;%
  top_n(-1, year ) %&gt;%
  summarize(base_rent = median(price))


rent_cum_change &lt;- rent %&gt;%
  filter(beds &lt; 3,
         city %in% top12_cities) %&gt;%
  group_by(beds, city, year) %&gt;%
  summarize(med_rent = median(price)) %&gt;% 
  left_join(base_years, by = c(&#39;beds&#39;,&#39;city&#39;)) %&gt;%
  mutate(index = med_rent/base_rent * 100)

head(rent_cum_change)</code></pre>
<pre><code>## # A tibble: 6 × 6
## # Groups:   beds, city [1]
##    beds city      year med_rent base_rent index
##   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1     0 berkeley  2001    1148.     1148. 100  
## 2     0 berkeley  2002    1000      1148.  87.1
## 3     0 berkeley  2003     888.     1148.  77.3
## 4     0 berkeley  2004     825      1148.  71.9
## 5     0 berkeley  2005     850      1148.  74.1
## 6     0 berkeley  2006     900      1148.  78.4</code></pre>
<pre class="r"><code>ggplot(rent_cum_change) +
  aes(x = year, y = index, color = city) +
  geom_line(show.legend = FALSE) + 
  facet_grid( beds ~ city ) +
  labs(
    title = &#39;Cumulative % change in 0, 1, and 2-bed rentals in Bay Area&#39;,
    subtitle = &#39;2000-2018&#39;,
    x = NULL,
    y = NULL,
    caption = &#39;Tidy Tuesday Spotify Songs dataset (accessed 04-09-2022)&#39;
  )</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-10-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="challenge-2-2016-california-contributors-plots" class="section level1">
<h1>Challenge 2: 2016 California Contributors plots</h1>
<p>We are looking to create a plot to analyse the origin of the contributions to the campaigns of Hillary Clinton and Donald Trump in the 2016. This we will do by joining a dataset with the names of the cities and their zipcodes with a dataset featuring the contributions of each of the candidates.</p>
<p><img src="../../images/challenge2.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(tidytext)
# Make sure you use vroom() as it is significantly faster than read.csv()
CA_contributors_2016 &lt;- vroom::vroom(here::here(&quot;data&quot;,&quot;CA_contributors_2016.csv&quot;))
zipcodes &lt;- vroom::vroom(here::here(&quot;data&quot;,&quot;zip_code_database.csv&quot;))</code></pre>
<pre class="r"><code>head(CA_contributors_2016)</code></pre>
<pre><code>## # A tibble: 6 × 4
##   cand_nm                 contb_receipt_amt   zip contb_date
##   &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;    
## 1 Clinton, Hillary Rodham              50   94939 2016-04-26
## 2 Clinton, Hillary Rodham             200   93428 2016-04-20
## 3 Clinton, Hillary Rodham               5   92337 2016-04-02
## 4 Trump, Donald J.                     48.3 95334 2016-11-21
## 5 Sanders, Bernard                     40   93011 2016-03-04
## 6 Trump, Donald J.                    244.  95826 2016-11-24</code></pre>
<pre class="r"><code>head(zipcodes)</code></pre>
<pre><code>## # A tibble: 6 × 16
##   zip   type     primary_…¹ accep…² unacc…³ state county timez…⁴ area_…⁵ latit…⁶
##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 00501 UNIQUE   Holtsville &lt;NA&gt;    I R S … NY    Suffo… Americ…     631    40.8
## 2 00544 UNIQUE   Holtsville &lt;NA&gt;    Irs Se… NY    Suffo… Americ…     631    40.8
## 3 00601 STANDARD Adjuntas   &lt;NA&gt;    Colina… PR    Adjun… Americ…  787939    18.2
## 4 00602 STANDARD Aguada     &lt;NA&gt;    Alts D… PR    &lt;NA&gt;   &lt;NA&gt;        787    18.4
## 5 00603 STANDARD Aguadilla  Ramey   Bda Ca… PR    Aguad… Americ…     787    18.4
## 6 00604 PO BOX   Aguadilla  Ramey   &lt;NA&gt;    PR    &lt;NA&gt;   &lt;NA&gt;         NA    18.4
## # … with 6 more variables: longitude &lt;dbl&gt;, world_region &lt;chr&gt;, country &lt;chr&gt;,
## #   decommissioned &lt;dbl&gt;, estimated_population &lt;dbl&gt;, notes &lt;chr&gt;, and
## #   abbreviated variable names ¹​primary_city, ²​acceptable_cities,
## #   ³​unacceptable_cities, ⁴​timezone, ⁵​area_codes, ⁶​latitude</code></pre>
<pre class="r"><code># inspecting the dataframes to find out what the columns look like &amp; how to join them later</code></pre>
<pre class="r"><code># preparing zipcodes dataset for merging --&gt; change type of zip-column to double
zipcodes &lt;- zipcodes %&gt;%
  mutate(zip = as.double(zip)) %&gt;%
  select(zip, primary_city) #selecting only relevant columns

# combining the 2 datasets by zip code
contributors_by_city &lt;- left_join(CA_contributors_2016, zipcodes, by = &#39;zip&#39;)

#taking a look to see if the join was successful
head(contributors_by_city)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   cand_nm                 contb_receipt_amt   zip contb_date primary_city
##   &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;       
## 1 Clinton, Hillary Rodham              50   94939 2016-04-26 Larkspur    
## 2 Clinton, Hillary Rodham             200   93428 2016-04-20 Cambria     
## 3 Clinton, Hillary Rodham               5   92337 2016-04-02 Fontana     
## 4 Trump, Donald J.                     48.3 95334 2016-11-21 Livingston  
## 5 Sanders, Bernard                     40   93011 2016-03-04 Camarillo   
## 6 Trump, Donald J.                    244.  95826 2016-11-24 Sacramento</code></pre>
<pre class="r"><code>candidates &lt;- contributors_by_city %&gt;%
  group_by(cand_nm) %&gt;% 
  summarise(total_contr = sum(contb_receipt_amt)) %&gt;%
  top_n(10, total_contr)

candidates &lt;- candidates$cand_nm

total_contb_by_city &lt;- contributors_by_city %&gt;%
  group_by(cand_nm, primary_city) %&gt;%
  summarize(total_contr = sum(contb_receipt_amt)) %&gt;% 
  ungroup %&gt;% group_by(cand_nm) %&gt;% top_n(10) %&gt;%
  ungroup

candidates</code></pre>
<pre><code>##  [1] &quot;Bush, Jeb&quot;                 &quot;Carson, Benjamin S.&quot;      
##  [3] &quot;Clinton, Hillary Rodham&quot;   &quot;Cruz, Rafael Edward &#39;Ted&#39;&quot;
##  [5] &quot;Fiorina, Carly&quot;            &quot;Kasich, John R.&quot;          
##  [7] &quot;Paul, Rand&quot;                &quot;Rubio, Marco&quot;             
##  [9] &quot;Sanders, Bernard&quot;          &quot;Trump, Donald J.&quot;</code></pre>
<pre class="r"><code>total_contb_by_city %&gt;% 
  filter(cand_nm %in% candidates) %&gt;% # selecting relevant candidates
  # ordering the values by size of contribution
  mutate(cand_nm = as.factor(cand_nm),
        primary_city = reorder_within(primary_city, total_contr, cand_nm)) %&gt;%
  ggplot() + # plotting the graph
  aes(primary_city, total_contr, fill = cand_nm) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~cand_nm, scales = &#39;free&#39;) +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL,
         y = &quot;Amount raised&quot;,
         title = &quot;Where did candidates raise the most money&quot;) +
  scale_y_continuous(labels = scales::dollar_format())</code></pre>
<p><img src="/blogs/homework1_files/figure-html/unnamed-chunk-14-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="details" class="section level1">
<h1>Details</h1>
<ul>
<li>Who did you collaborate with: Samarth Sharma, Vivian van Oosten, Anastasia Fu, Jaelyn Shi, Andrew Robak, Shivant Maharaj</li>
<li>Approximately how much time did you spend on this problem set: 12 hours</li>
<li>What, if anything, gave you the most trouble: Figuring out GIT to be able to work together effectively.</li>
</ul>
</div>
